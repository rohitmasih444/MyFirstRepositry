{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8fb9a3",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31691d",
   "metadata": {},
   "source": [
    "-  # Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c842ba5",
   "metadata": {},
   "source": [
    "Linear rgression is one of the most fundamental algorithms in the Machine Learning world. It is the door to the magical world ahead. But before proceeding with the algorithm, let's first disuss the lifeCycle of any machine learning models. This diagram explains the creation of a Machine Learning model from scratch and then taking the same model further with hyperparameter tuning to increase its accuracy, deciding the deployment strategies for that model and once deployed setting up the logging and monitoring framwork to generate reports and dashboards based on the client requirements. A typically lifeCycle diagram for a machine learning model looks like: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936958f1",
   "metadata": {},
   "source": [
    "![](adi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc754",
   "metadata": {},
   "source": [
    "![](MAchine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39548178",
   "metadata": {},
   "source": [
    "Now, Let's take our discussion of Linear Regression further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52b9fe",
   "metadata": {},
   "source": [
    "# What is Regression Analysis?\n",
    "Regression is statistics is the process of predicting a Lable(or Dependent Variable) based on the features(Independent Variables) at hand. Regression is used for time series modeling and finding the causal effect relationship between the variable and forecasting. For example, the relationship between the stock prices of the company and various factors like customer reputation and company annual performence etc. Can be studied using regression.\n",
    "\n",
    "Regression analysis is an important tool for analysing and modeling data. Here, we fit a curve/line to the data points, in such manner that the differnces between the distance of the actual data points from the plotted curve/line is minimum. The topic will be explained in detail in the coming sections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631bb8b",
   "metadata": {},
   "source": [
    "## The use of Regression\n",
    "Regression analyses the relationship between two or more features. Let's take an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdf30b",
   "metadata": {},
   "source": [
    "Let's suppose we went to make an application which predicts the chances of addmission a student to a forein university."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bed89a",
   "metadata": {},
   "source": [
    "The benefits of using Regression analysis are as follows:\n",
    "-  It shows the significant relationships between the lable (dependent variable) and the features(independent variable).\n",
    "-  It shows the extent of the impact of multiple independent variable on the dependent variable\n",
    "-  It can also measure these effects even if the variables are on a difference scale.\n",
    "\n",
    "These features enable the data scientists to find the best set of independent variables for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e9349",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Linear Regression is one of the most fundamental and widly Machine Learning Algorithms which people start with.\n",
    "Building blocks of a Linear Regression model are:\n",
    "\n",
    "-  Discreet/continuous independent variables\n",
    "-  A best-fit regression line\n",
    "-  Continuous dependent variable, i.e. A linear Regression model predicts the dependent variable using a regression line based on the indenpendent variables. The equation of the Linear Regression is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8b480",
   "metadata": {},
   "source": [
    "$$Y=a+b*x + e$$ , $$Y=mx+c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf09c3",
   "metadata": {},
   "source": [
    "Where, a is the intercept, b is the slope of the line, and e is the error term. The equation above is used to predict the value of the target variable based on the given predictor variables(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3aecb4",
   "metadata": {},
   "source": [
    "### The problem statement:\n",
    "\n",
    "This data is about the amount spent in advertising through different channels like TV, Radio and Newspaper. The goal is to predict how the expence on each chennal affects the sale and is there a way to optimise that sale??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11713205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Library\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ea579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'https://raw.githubusercontent.com/training-ml/Files/main/Advertising.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04de9c4",
   "metadata": {},
   "source": [
    "what are the **features**?\n",
    "\n",
    "-  TV: Advertising dollars spent on TV for a single product in a given market(in thousands of dollars)\n",
    "-  Radio: Advertising dollars spent on Radio\n",
    "-  Newspaper: Advertising dollars spent on Newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774f692",
   "metadata": {},
   "source": [
    "What is the **response**?\n",
    "-  Sale: Sale of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bed347",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # print the summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum() # finding the count of missing values from different columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cddcf",
   "metadata": {},
   "source": [
    "Now, let's showcase the relationship between the feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between the features and the response using scatterplots\n",
    "\n",
    "fig, axs = plt.subplots(1 ,3)\n",
    "\n",
    "data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(12,6))\n",
    "data.plot(kind='scatter', x='radio', y='sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2])\n",
    "fig.savefig('testdata.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea8849",
   "metadata": {},
   "source": [
    "## Question about the data\n",
    "A generic question shall be: How the company should optimise the spends on advertising to maximise the sale?\n",
    "\n",
    "\n",
    "\n",
    "These general question might lead you to more specific questions:\n",
    "\n",
    "   1. What's the relationship between ads and sales\n",
    "   2. How prominent is that relationship ?\n",
    "   3. Which ad types contribute to sales?\n",
    "   4. How each ad contributes to sales?\n",
    "   5. Can sales be predicted based on the expense of the advertisement?\n",
    "   \n",
    "We will explore these questions below!!\n",
    "\n",
    "From the relationship diagram above, it can be observed that there seems to be a linear relationship between the features TV ad, Radio ad and the sales is almost a linear one. A linear relationship typically looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84167278",
   "metadata": {},
   "source": [
    "```\n",
    "Hence, we can build a model using the linear Regression Algorithm.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223bac1",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e158e",
   "metadata": {},
   "source": [
    "https://www.desmos.com/calculator/2rnqgoa6a4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b67a99",
   "metadata": {},
   "source": [
    "Simple Linear regression is a method for predicting a **quantitavie response** using a **single feature**(\"input variable\"), The mathematical equation is;\n",
    "\n",
    "$y=\\beta_0 + \\beta_ix$\n",
    "\n",
    "Whats do terms represents?\n",
    "\n",
    "-  y is the response or the target variable\n",
    "-  x is the feature\n",
    "-  $\\beta$ is the coefficient of x\n",
    "-  $\\beta_0$ is the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0a7d9",
   "metadata": {},
   "source": [
    "$\\beta$ and $\\beta_1$ are the **model coefficients**, To create a model, we must \"learn\" the values of these coefficients. And once we have the value of these coefficients,  we can use the model to predict the sales!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f88f",
   "metadata": {},
   "source": [
    "**Estimating (\"Learning\") Model coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc41971",
   "metadata": {},
   "source": [
    "The coefficients are estimated using the **least-squares criterion**, i.e., the fit line has to be calculated that minimizes the **sum of squared residiuals** (or \"sum of squared eerors\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615134e2",
   "metadata": {},
   "source": [
    "### The mathematical involved\n",
    "\n",
    "Take a quick look at the plot created. Now consider each point, and know that each of them has a coordinate in the form(X,Y). Now drow an imaginary line between each point and the current \"best-fit\" line, We'll call the distance between each point and the current best-fit line as D. To get a quick image of what we're trying to visualize, take a look at the picture below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47cb26",
   "metadata": {},
   "source": [
    "![](Linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d209834",
   "metadata": {},
   "source": [
    "What elements are present in the diagram??\n",
    "\n",
    "-  The red points are the **observed value** of x and y.\n",
    "-  The blue line in the **least squares line**.\n",
    "-  The green lines are the **residuals**, which is the distance between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955dd42",
   "metadata": {},
   "source": [
    "The general equetion of a straight line is:\n",
    "\n",
    "$$y=mx+c$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2b093",
   "metadata": {},
   "source": [
    "it means that if we have the value of m and b, we can predict all the values of y corresponding x. During construction of a Linear Regression Model, the computer tries to calculate the values of m and b to get a straight line. But the qustion is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b783b",
   "metadata": {},
   "source": [
    "How Do you know this is the best-fit line??\n",
    "The best-fit line is obtained by minimizing the residual, Residual is the distance between the actual Y and the predicted Y, as show below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76a34d",
   "metadata": {},
   "source": [
    "![](Residual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3802f",
   "metadata": {},
   "source": [
    "Mathematically, Residual is:\n",
    "$$r=y-(mx+c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e60ca",
   "metadata": {},
   "source": [
    "Hence, the sum of the square of residuals is:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$r_i=y_i - (mx_i+b)$$  (Residual for one point)\n",
    "$$\\sum_{i=1}^n r_i=\\sum_{i=1}^n(y_i - (mx+c))$$   (sum of residuals)\n",
    "\n",
    "$$R(x)=\\sum_{i=1}^n r_i^2=>\\sum^n(y_i - (mx_i +b))^2$$ (sum of squares of residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7bdad",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "But as the residual's minima is dependents on two variables m and b, it becomes a *Paraboloid* and the appropriate m and b are calculating using_*Gradient Descent*_ as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70fd46",
   "metadata": {},
   "source": [
    "![](Descent.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X = feature and y = label\n",
    "\n",
    "x = data[['TV']]\n",
    "y = data.sales\n",
    "\n",
    "# follow the usual sklearn pattern: import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print intercept and coefficients\n",
    "print(lm.intercept_)  ## this is C\n",
    "print(lm.coef_)   ## this is m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ed519",
   "metadata": {},
   "source": [
    "### Interpreting the model\n",
    "How do we interpret the coefficient for spends on TV ad($\\beta_1$)\n",
    "\n",
    "-  A \"unit\" increase in spends on a TV ad is **associated with** a 0.047537 \"unit\" increase in sales.\n",
    "-  Or, an additional $1,000 on TV ads is **tranlate to** an increase in sales by 47.53 Dollars.\n",
    "\n",
    "As an increase in TV ad expenditure is associated with a **descent** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340deae7",
   "metadata": {},
   "source": [
    "### Prediction using the model\n",
    "\n",
    "    if the expense on TV as is $50000, what will be the sales prediction for the market?\n",
    "$$y=\\beta_0+\\beta_1x$$\n",
    "$$y = 7.032594+0.047537$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47362127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction\n",
    "7.032594 + 0.047537*50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41188a7b",
   "metadata": {},
   "source": [
    "Thus, we would predict sales of 9,409 **widgets** in that market\n",
    "\n",
    "Let's do the same thing using code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deae0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame since the model expects it\n",
    "# X_new = pd.DataFrame({'TV':[50]})\n",
    "# X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on a new value\n",
    "lm.predict([[50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2510f",
   "metadata": {},
   "source": [
    "### How Least Squares Line changes based on input data (Just for Demo purposem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382ca17",
   "metadata": {},
   "source": [
    "### https://www.desmos.com/calculator/jmwquvmikhr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca6b5c",
   "metadata": {},
   "source": [
    "## Model confidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09fcb3",
   "metadata": {},
   "source": [
    "### How well Does the Model Fit the data ?\n",
    "One of the most generic way to evaluate the fit of a linear model is by computing the **R-squared** value. R-squared explains the **proportion of variance**, i.e. the proportion of variance in the observed data which the model explains, or the redution in error over the **null model**. (A null model only predicts the mean of the observed  responses, and thus it only has an intercept and no slope.)\n",
    "\n",
    "The value of R-squared lies between 0 and 1. And value closer to 1 is better as it means that more variance is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a35588",
   "metadata": {},
   "source": [
    "### $R^2$ Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c4100",
   "metadata": {},
   "source": [
    "The R-squared statistic provides a measure of fit. it takes the form of a proportion-the proportion of variance explained-and so it always takes on a value between 0 and 1. In simple words, it represents how much of our data is being explained by our model. For example, $R^2$ statistic = 0.75, it says that our model fits 75% of the total data set. Similarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation. Mathematically $R^2$ statistic is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a220ea",
   "metadata": {},
   "source": [
    "$$R^2= \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dc2f5",
   "metadata": {},
   "source": [
    "Where RSS: is the Residuals Sum of squares and is given as:\n",
    "$$RSS=\\sum_{i=1}^n(y_i-\\hat y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea23cc",
   "metadata": {},
   "source": [
    "RSS is the residual(error) term we have been talking about so far. And, TSS: is the Total Sum of squares and given as:\n",
    "$$TSS= \\sum(y_i-\\bar y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9afc5",
   "metadata": {},
   "source": [
    "TSS is calculated when we consider the line passing through the mean value of y, to be the best-fit line. Just like RSS, we calculate the error term when the best-fit line is the line passing through the mean value of y and we get the value of TSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfad923",
   "metadata": {},
   "outputs": [],
   "source": [
    "Photo Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b09173",
   "metadata": {},
   "source": [
    "#### Metric to check model performance(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70590e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all TV records and predict sales\n",
    "predicted_sales = lm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare original sales numbers with our  model predicted sales\n",
    "\n",
    "r2_score(y_true=y, y_pred=predicted_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade813b3",
   "metadata": {},
   "source": [
    "### Adjusted $R^2$ statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2b18f",
   "metadata": {},
   "source": [
    "As we increase the number of independent variables in our quation, the $R^2$ increase as well. But that doesn't mean that the new independent variables have any correlation with the output variable. In other words, even eith the addition of new features in our model, it is not necessary that our model will yield better results but $R^2$ value will increase. To rectify this problem, we use Adjusted $R^2$ value which penalises excessive use of such features whcih do not correlate with the outpuut data. Let's understand this with an example:\n",
    "\n",
    "\n",
    "We can see that $R^2$ always increase with an increase in the number of independent variables. Thus, it doesn't give a better picture and so we need Adjusted $R^2$ value to keep this in check. Mathematically, it is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1a8dc",
   "metadata": {},
   "source": [
    "$$R^2_{adjusted} = 1- \\frac{(1-R^2)(N-1)}{N-P-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae87687",
   "metadata": {},
   "source": [
    "where, $R^2$ = sample R-square<br>\n",
    "       P = Number of predictors<br>\n",
    "       N = Total sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edafa9",
   "metadata": {},
   "source": [
    "In the equation above, when p=0, we can see that adjusted R2 becomes equal to R2. Thus, adjusted R2 will always be less than or equal to R2, and it pepalises the excess of independent variables which do not affect the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc61104",
   "metadata": {},
   "source": [
    "It is \"Good\" R-Squared value? Now, that's hard to say. In reality , the domain to which the data belongs to plays a significant role in deciding the thershold for the R-Squared value. Therefore, it's a tool for **comparing Linear Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4854d93",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "Till now, we have created the model based on only one features. Now, we'll include multiple features and create a model to see the relationshp between those features and the label column. This is called **Multiple Linear Regression**\n",
    "\n",
    "$y=\\beta_0 + \\beta_1 x_1 + ...+\\beta_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36807e1",
   "metadata": {},
   "source": [
    "Each x represnts a difference features, and each features has its own coefficient. In this case:\n",
    "\n",
    "$y=\\beta_0+\\beta_1 \\times TV+ \\beta_2 \\times Radio + \\beta_3\\times Newspaper$\n",
    "    \n",
    "Let's use Statsmodel to estimate these coefficiwnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y\n",
    "\n",
    "x = data[['TV', 'radio', 'newspaper']]\n",
    "y = data.sales\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.fit(x,y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "\n",
    "print('Intercept : ->', lm.intercept_)\n",
    "print('TV :        ->', lm.coef_[0])\n",
    "print('Radio :     ->', lm.coef_[1])\n",
    "print('Newspaper : ->', lm.coef_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a6020",
   "metadata": {},
   "source": [
    "How do we intercept these coefficients??  If we look at the coefficients, the coefficient for the newspaper spends in negative. It means that hte money spent for newspaper advertisements it not contributing in a positive way to the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366d1f6",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "How do i decide **which features have to be included** in a linear model? Here's one idea;\n",
    "-  Check if the R-squared value goes up when you add new predictors to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb98c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for R2_score on TV and Radio as we can see they are positively related(Co-efficient)\n",
    "\n",
    "x=data[['TV', 'radio']]\n",
    "y=data.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420abd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's feed features and label and train the model\n",
    "lm.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the r2\n",
    "predicted_sales=lm.predict(x)\n",
    "r2_score(y,predicted_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do the same for all the features\n",
    "x= data[['TV', 'radio','newspaper']]\n",
    "y= data.sales\n",
    "\n",
    "# trainig \n",
    "lm.fit(x,y)\n",
    "\n",
    "# r2\n",
    "predicted_sales = lm.predict(x)\n",
    "r2_score(y, predicted_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3191caf",
   "metadata": {},
   "source": [
    "Selecting the model with the highest value of R-squared is not a correct approach as the value of R-squared shall always increase whenever a new features is taken for cosidration even if the features is unrelated to the response.\n",
    "\n",
    "The alternative is to use **adjusted R-squared** whcih penalises the model complaxity (to control overfitting), but this again generally **under-penalises complexity.**\n",
    "\n",
    "a better approach to features selection is **Cross-vailation**. It provides a more reliable  way to choose which of the created models will best **generalise** as it better estimates of out-of-sample error. An advantage is that the cross-validation method can be applied to any machine learning model and the scikit-learn package provides extensive functionality for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d3352",
   "metadata": {},
   "source": [
    "## Project_1 with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad05f6",
   "metadata": {},
   "source": [
    "``Problem Statment``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeeedf",
   "metadata": {},
   "source": [
    "### **We need to predict the chance of admission based on the students various scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7a28d",
   "metadata": {},
   "source": [
    "Features or independent variables\n",
    "\n",
    "-  GRE Score\n",
    "-  TOEFL Score\n",
    "-  University Rating\n",
    "-  SOP\n",
    "-  LOR\n",
    "-  CGPA\n",
    "-  Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028baa4",
   "metadata": {},
   "source": [
    "#### Label/Target\n",
    "-  Chance of Admit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50c0ea",
   "metadata": {},
   "source": [
    "##### Always refer sklearn official documentation if you are not sure about syntax or parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f150947",
   "metadata": {},
   "source": [
    "#### https://scikit-learn.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214387ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with importing necessary librabries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ce67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'https://raw.githubusercontent.com/training-ml/Files/main/Admission_Prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f551d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unwanted column\n",
    "data=data.drop('Serial No.', axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d91032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ec617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand data at high level. Check the statistics of data set\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the null values\n",
    "data['University Rating']=data['University Rating'].fillna(data['University Rating'].mode()[0]) # Mode only for categorical data\n",
    "\n",
    "data['TOEFL Score'] = data['TOEFL Score'].fillna(data['TOEFL Score'].mean())\n",
    "\n",
    "data['GRE Score'] = data['GRE Score'].fillna(data['GRE Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0923238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if NaN's are filled\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to verify NaN's are filled\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['University Rating'].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ddf90",
   "metadata": {},
   "source": [
    "Now the data looks good and there are no missing values. Also, the first column is just serial numbers, so we don't need that column. let's drop it from data and make it more clean.\n",
    "\n",
    "Let's visualize the data and analyze the relationship between independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa16c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15), facecolor='red')\n",
    "plotnumber=1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=8:\n",
    "        ax=plt.subplot(2,4, plotnumber)\n",
    "        sns.distplot(data[column])\n",
    "        plt.xlabel(column, fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3deac",
   "metadata": {},
   "source": [
    "The data distribution looks decent enough and there doesn't seem to be any skewness. Great let's go ahead!\n",
    "\n",
    "Let's observe the relationship between indepedent variables and dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c346a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data set into features and label\n",
    "\n",
    "y=data['Chance of Admit']\n",
    "x=data.drop('Chance of Admit', axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0567ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10), facecolor='yellow')\n",
    "plotnumber=1\n",
    "\n",
    "for column in x:\n",
    "    if plotnumber<=8:\n",
    "        ax=plt.subplot(2,4,plotnumber)\n",
    "        plt.scatter(x[column], y)\n",
    "        plt.xlabel(column, fontsize=10)\n",
    "        plt.ylabel('Chance of Admit', fontsize=10)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c97da",
   "metadata": {},
   "source": [
    "Great, the relationship between the dependent and independent variables look fairly linear. Thus, our linearity assumption is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff42e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaling, Formula z = (x - mean)/std\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_scaler = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e00216",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0405008",
   "metadata": {},
   "source": [
    "#### Example for StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get help within notebook and refer whenever you have any confusion.\n",
    "\n",
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab2b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "984edf66",
   "metadata": {},
   "source": [
    "Let's go ahead and use linear regression and see how good it fits our data. But first, Let's split our data in train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63cc74",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd983e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test. Model will be built on training data and tested on test data.\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x_scaler, y, test_size = 0.25, random_state=348)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70a4f9",
   "metadata": {},
   "source": [
    "#### Model instantiating and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "\n",
    "regression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa42a0",
   "metadata": {},
   "source": [
    "### Predict the chance of admission given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have already fit the scaler, you can transform the data\n",
    "print('Chance of Admit : ',regression.predict(scaler.transform([[327.0, 113.0, 4.0, 4.5, 4.5,9.04,0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc917466",
   "metadata": {},
   "source": [
    "### You can save the model and later you can use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6460d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to the local file system\n",
    "pickle.dump(regression, open('reg_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cec880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the saved model\n",
    "loaded_model = pickle.load(open('reg_model', 'rb'))\n",
    "\n",
    "a=loaded_model.predict(scaler.transform([[314,103,2,2,3,8.21,0]]))\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac815b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try oin new window with only pickle model\n",
    "scaler.transform([[314,103,2,2,3,8.21,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69e3c4",
   "metadata": {},
   "source": [
    "#### Let's check how well model fits on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted R2 score\n",
    "regression.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3460e24",
   "metadata": {},
   "source": [
    "#### Let's check how well model fits the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4fb86",
   "metadata": {},
   "source": [
    "#### Let's plot and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regression.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel('Actual Chance of Admission')\n",
    "plt.ylabel('Predicted Chance of Admission')\n",
    "plt.title('Actual vs model predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36db5eb",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "-  ``Mean absolute error (MAE)``: Represents average error\n",
    "\n",
    "\n",
    "\n",
    "-  ``Mean squared error (MSE)``: Similar to MAE but noise is exaggerated and larger errors are \"punished\". It is harder to interpret than MAE as it's not in base units, however, it is generally more popular.\n",
    "\n",
    "\n",
    "-  ``Root mean squared error``:Most popular metric, similar to MSE, however, the result is square rooted to make it more interpretable as it's in base units. It is recommended that RMSE be used as the primary metric to interpret your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regression.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439b94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RMSE\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d7f5d",
   "metadata": {},
   "source": [
    "## You have succesfully completed building Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c27f16",
   "metadata": {},
   "source": [
    "Now let's check if our model is overfitting our data using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d2f49",
   "metadata": {},
   "source": [
    "# Class21 jan2023 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238525f2",
   "metadata": {},
   "source": [
    "#### Let's see if our model is overfitting our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82415e",
   "metadata": {},
   "source": [
    "### ** Regularization\n",
    "\n",
    "When we use regression model to train some data, there is a good chance that the model will overfit the given training data set. Regularization helps sort this overfitting problem by restricting the degree of freedom of a equation i.e. simply reducing the number of degrees of a polynomial function by reducing their corresponding weights.\n",
    "In a linear equation, we do not want huge weights/coefficients as a small change in weight can make a large difference for the dependent variable(Y). So, regularization constraints the weights of such features to avoid overfitting.\n",
    "\n",
    "\n",
    "To regularize the model, a Shrinkage penaltiy is added to the cost function. Let's see different types of regularization in regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f227f2",
   "metadata": {},
   "source": [
    "-  LASSO\n",
    "-  RIDGE\n",
    "-  ELASTICNET(Less popular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac04d26",
   "metadata": {},
   "source": [
    "### LASSO(Least Absolute Shrinkage and Selection Operator) Regression (L1 form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285951e",
   "metadata": {},
   "source": [
    "LASSO regression penalize the model based on the sum of magnitude of the coefficients. The regularization term is given by\n",
    "\n",
    "regularization = $\\lambda * \\sum |\\beta_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c89e8",
   "metadata": {},
   "source": [
    "Where, $\\lambda$ is the shrinkage factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb8ccb",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 From)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312745fb",
   "metadata": {},
   "source": [
    "Ridge regression penalize the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by \n",
    "\n",
    "regularization = $\\lambda *\\sum |\\beta_{j}^2|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5f62c",
   "metadata": {},
   "source": [
    "Where, $\\lambda$ is the shrinkage factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074250be",
   "metadata": {},
   "source": [
    "### Difference between Ridge and LASSO\n",
    "\n",
    "Ridge regression shrinks the coefficients for those predictors which contribute very less in the model but have huge weights, very close to zero. But it never makes them exactly zero. Thus, the final model will still contain all those predictors, though with less weights. This doesn't help in interpreting the model very well. This is where LASSO regression differs with Ridge regression. In LASSO, the L1 penalty does reduce some coefficients exactly to zero when we use a suffuciently large tuning parameter $\\lambda$. So, in addition to regularizing, LASSO also performs featurs selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948121c3",
   "metadata": {},
   "source": [
    "### Why use Regularization? \n",
    "\n",
    "Regularization helps to reduce the variance of the model, without a substantial increase in the bias. If there is variance in the model that means that model won't fit well for dataset different than training data. The tunig parameter $\\lambda$ controls this bias and variance tradeoff. When the value of $\\lambda$ is increased up to a certain limit, it reduces the variance without losing any important properties in the data. But after a certain limit, the model will start losing some important properties which will increase the bias in the data. Thus, the selection of good value of $\\lambda$ is the key. The value of $\\lambda$ is selected using cross-validation methods. A set of $\\lambda$ is selected and cross-validation error is calculated for each value of $\\lambda$ and that value of $\\lambda$ is selected for which the cross-validation error is minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c26c0",
   "metadata": {},
   "source": [
    "### Lasso Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV will return best alpha after max iteration\n",
    "# Normalize is subtracting the mean and dividing by the L2- form\n",
    "\n",
    "lasscv = LassoCV(alphas=None, max_iter=10, normalize=True)\n",
    "lasscv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha parameter\n",
    "alpha=lasscv.alpha_\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have best parameter, let's use Lasso regression and see well our data has fitted before\n",
    "\n",
    "lasso_reg=Lasso(alpha)\n",
    "lasso_reg.fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934b505",
   "metadata": {},
   "source": [
    "### Using Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e78b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.001,0.1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cff8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV will return best alpha and coefficients after performing 10 cross validations.\n",
    "ridgecv=RidgeCV(alphas=np.arange(0.001,0.1,0.01), normalize=True)\n",
    "\n",
    "ridgecv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ac8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model=Ridge(alpha=ridgecv.alpha_)\n",
    "ridge_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2615b",
   "metadata": {},
   "source": [
    "We got around same r2 square using Ridge regression as well. So, it's safe to say there is no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0418957",
   "metadata": {},
   "source": [
    "So, we can see by using different type of regularization, we still are getting the same r2 score. That means our OLS model has been well trained over the training data and there is no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa3035",
   "metadata": {},
   "source": [
    "#### Let's see the underlying assumptions:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2c80a",
   "metadata": {},
   "source": [
    "-  The regression model is linear in terms of coefficients and error term.\n",
    "-  The mean of the residulas is Zero.\n",
    "-  THe error terms are not correlated with each other, i.e. given an error value; we cannot predict the next error value.\n",
    "-  No Multicollinearity, i.e. no independent variable should be correlated with each other or affect one another. If there is multicollinearity, the precision of prediction by the OLS model decreases.\n",
    "-  The error terms are normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9917dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
